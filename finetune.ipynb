{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "from numpy import savez_compressed\n",
    "from numpy import asarray, load, expand_dims\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, Normalizer\n",
    "from sklearn.svm import SVC\n",
    "from matplotlib import pyplot\n",
    "from mtcnn import MTCNN\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "model = load_model('Serialized/facenet_keras.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'input_1:0' shape=(?, 160, 160, 3) dtype=float32>]\n",
      "[<tf.Tensor 'Bottleneck_BatchNorm/cond/Merge:0' shape=(?, 128) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "print(model.inputs)\n",
    "print(model.outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face(filename, required_size=(160, 160)):\n",
    "    image = Image.open(filename)\n",
    "    image = image.convert('RGB')\n",
    "    pixels = asarray(image)\n",
    "    detector = MTCNN()\n",
    "    results = detector.detect_faces(pixels)\n",
    "    x1, y1, width, height = results[0]['box']\n",
    "    x1, y1 = abs(x1), abs(y1)\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    face = pixels[y1:y2, x1:x2]\n",
    "    image = Image.fromarray(face)\n",
    "    image = image.resize(required_size)\n",
    "    face_array = asarray(image)\n",
    "    return face_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_faces(directory):\n",
    "    faces = list()\n",
    "    for filename in listdir(directory):\n",
    "        path = directory + filename\n",
    "        face = extract_face(path)\n",
    "        faces.append(face)\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(directory):\n",
    "    X, y = list(), list()\n",
    "    for subdir in listdir(directory):\n",
    "        path = directory + subdir + '/'\n",
    "        faces = load_faces(path)\n",
    "        labels = [subdir for _ in range(len(faces))]\n",
    "        print('>loaded %d examples for class: %s' % (len(faces), subdir))\n",
    "        X.extend(faces)\n",
    "        y.extend(labels)\n",
    "    return asarray(X), asarray(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loaded 0 examples for class: .ipynb_checkpoints\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      ">loaded 14 examples for class: ben_afflek\n",
      ">loaded 21 examples for class: jerry_seinfeld\n",
      ">loaded 19 examples for class: madonna\n",
      ">loaded 17 examples for class: elton_john\n",
      ">loaded 22 examples for class: mindy_kaling\n",
      ">loaded 0 examples for class: .ipynb_checkpoints\n",
      ">loaded 5 examples for class: ben_afflek\n",
      ">loaded 5 examples for class: jerry_seinfeld\n",
      ">loaded 5 examples for class: madonna\n",
      ">loaded 5 examples for class: elton_john\n",
      ">loaded 5 examples for class: mindy_kaling\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y = load_dataset('data/train/')\n",
    "test_X, test_y = load_dataset('data/val/')\n",
    "savez_compressed('5-celebrity-faces-dataset.npz', \n",
    "                  train_X, \n",
    "                  train_y,\n",
    "                  test_X,\n",
    "                  test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(model, face_pixels):\n",
    "    face_pixels = face_pixels.astype('float32')\n",
    "    mean, std = face_pixels.mean(), face_pixels.std()\n",
    "    face_pixels = (face_pixels - mean) / std\n",
    "    samples = expand_dims(face_pixels, axis=0)\n",
    "    yhat = model.predict(samples)\n",
    "    return yhat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load('5-celebrity-faces-dataset.npz')\n",
    "train_X, train_y, test_X, test_y = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n",
    "model = load_model('Serialized/facenet_keras.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_train_X = list()\n",
    "for face_pixels in train_X:\n",
    "    embedding = get_embedding(model, face_pixels)\n",
    "    embedded_train_X.append(embedding)\n",
    "embedded_train_X = asarray(embedded_train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_test_X = list()\n",
    "for face_pixels in test_X:\n",
    "    embedding = get_embedding(model, face_pixels)\n",
    "    embedded_test_X.append(embedding)\n",
    "embedded_test_X = asarray(embedded_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "savez_compressed('5-celebrity-faces-embeddings.npz',\n",
    "                embedded_train_X,\n",
    "                train_y,\n",
    "                embedded_test_X,\n",
    "                test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load('5-celebrity-faces-embeddings.npz')\n",
    "train_X, train_y, test_X, test_y = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_encoder = Normalizer(norm='l2')\n",
    "train_X = in_encoder.transform(train_X)\n",
    "test_X = in_encoder.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_encoder = LabelEncoder()\n",
    "out_encoder.fit(train_y)\n",
    "train_y = out_encoder.transform(train_y)\n",
    "test_y = out_encoder.transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel='linear', probability=True)\n",
    "model.fit(train_X, train_y)\n",
    "filename = 'svm.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: train=100.000, test=100.000\n"
     ]
    }
   ],
   "source": [
    "yhat_train = model.predict(train_X)\n",
    "yhat_test = model.predict(test_X)\n",
    "score_train = accuracy_score(train_y, yhat_train)\n",
    "score_test = accuracy_score(test_y, yhat_test)\n",
    "print('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
